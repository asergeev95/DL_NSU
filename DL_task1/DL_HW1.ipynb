{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from src.helpers import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import Input\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense, concatenate, Activation, Dropout\n",
    "from keras.models import Model\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.pooling import GlobalMaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read initial data via parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_train, data_test = get_parsed_data()\n",
    "#dropout useless data and fill NaN with zeros\n",
    "data_train.drop(['date', 'id', 'twitid'], axis=1, inplace = True, errors='ignore')\n",
    "data_train.fillna(0)\n",
    "\n",
    "data_train['label'] = data_train.apply(define_label, axis=1)\n",
    "data_test['label'] = data_test.apply(define_label, axis=1)\n",
    "data_train['clear_text'] = data_train.text.apply(clear_text)\n",
    "data_test['clear_text'] = data_test.text.apply(clear_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(data_train.text)\n",
    "X_test = vectorizer.transform(data_test.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn log regression model training and predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7070143884892086"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, data_train.label)\n",
    "model.score(X_test, data_test.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vector model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "data = pd.concat([data_train, data_test], ignore_index=True)\n",
    "split_text = data.clear_text.apply(lambda sent: sent.lower().split())\n",
    "w2v_model = Word2Vec(sentences=split_text, sg=1, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transform data to index sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data.clear_text)\n",
    "X_train = tokenizer.texts_to_sequences(data_train.clear_text)\n",
    "X_test = tokenizer.texts_to_sequences(data_test.clear_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen=30)\n",
    "X_test = pad_sequences(X_test, maxlen=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embeding matrix creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, 100))\n",
    "oov = []\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in w2v_model.wv.vocab:\n",
    "        embedding_vector = w2v_model.wv.get_vector(word)\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        oov.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_input = Input(shape=(30,), dtype='int32')\n",
    "keys_num = len(list(w2v_model.wv.vocab.keys()))\n",
    "inp = Embedding(keys_num+1, 100, input_length=30,\n",
    "                weights=[embedding_matrix], trainable=False)(tweet_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "branches = []\n",
    "\n",
    "for size, filters_count in [(2, 10), (3, 10), (4, 10), (5, 10)]:\n",
    "    for i in range(filters_count):\n",
    "        branch = Conv1D(filters=1, kernel_size=size, padding='valid', activation='relu')(inp)\n",
    "        branch = GlobalMaxPooling1D()(branch)\n",
    "        branches.append(branch)\n",
    "x = concatenate(branches, axis=1) \n",
    "drop1 = Dropout(0.2)(x)\n",
    "hidden = Dense(30, activation='relu')(drop1)\n",
    "drop2 = Dropout(0.2)(hidden)\n",
    "out = Dense(3, activation='relu')(drop2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "model = Model(input=tweet_input, output=out) \n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18035/18035 [==============================] - 56s 3ms/step - loss: 0.8336 - acc: 0.6535\n",
      "Epoch 2/10\n",
      "18035/18035 [==============================] - 49s 3ms/step - loss: 0.7743 - acc: 0.6560\n",
      "Epoch 3/10\n",
      "18035/18035 [==============================] - 49s 3ms/step - loss: 0.7381 - acc: 0.6731\n",
      "Epoch 4/10\n",
      "18035/18035 [==============================] - 49s 3ms/step - loss: 0.7512 - acc: 0.6713\n",
      "Epoch 5/10\n",
      "18035/18035 [==============================] - 49s 3ms/step - loss: 0.7285 - acc: 0.6798\n",
      "Epoch 6/10\n",
      "18035/18035 [==============================] - 49s 3ms/step - loss: 0.6988 - acc: 0.7063\n",
      "Epoch 7/10\n",
      "18035/18035 [==============================] - 49s 3ms/step - loss: 0.7019 - acc: 0.7116\n",
      "Epoch 8/10\n",
      "18035/18035 [==============================] - 50s 3ms/step - loss: 0.6779 - acc: 0.7186\n",
      "Epoch 9/10\n",
      "18035/18035 [==============================] - 50s 3ms/step - loss: 0.6610 - acc: 0.7219\n",
      "Epoch 10/10\n",
      "18035/18035 [==============================] - 49s 3ms/step - loss: 0.6635 - acc: 0.7204: 0s - loss: 0.6630 - acc: 0.7 - ETA: 0s - loss: 0.6638 - acc: 0.7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1db44ba04e0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X_train], y=to_categorical(data_train.label.values+1), verbose=1, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5560/5560 [==============================] - 8s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8027225728944051, 0.6577338130354023]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, to_categorical(data_test.label.values+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = model.predict(X_test)\n",
    "predicted[len(predicted)-1] = [0,0,1]\n",
    "to_categorical([np.argmax(x) for x in predicted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44240855705378274"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(to_categorical([np.argmax(x) for x in predicted]), to_categorical(data_test.label.values+1), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6579136690647482"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(to_categorical([np.argmax(x) for x in predicted]), to_categorical(data_test.label.values+1), average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
